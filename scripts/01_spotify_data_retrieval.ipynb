{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import date, timedelta\n",
    "\n",
    "work_dir = '/home/jovyan/work/' #GPU server directory Duke Scavenger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download User Session Log dataset from the AI Crowd website page of Spotify Skip Prediction challenge\n",
    "!wget -c https://os.zhdk.cloud.switch.ch/swift/v1/crowdai-public/spotify-sequential-skip-prediction-challenge/20181113_training_set.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the dataset into files\n",
    "!tar xvzf 20181113_training_set.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download Songs dataset from the AI Crowd website page of Spotify Skip Prediction challenge\n",
    "!wget -c https://os.zhdk.cloud.switch.ch/swift/v1/crowdai-public/spotify-sequential-skip-prediction-challenge/20181120_track_features.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar xvzf 20181120_track_features.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access 'training_set/*20180720*.csv': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls -ltr training_set/*20180720*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!ls -ltr track_features/*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset from 15th July to 10th August"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = date(2018, 7, 15)\n",
    "end_date = date(2018, 8, 10)\n",
    "delta = timedelta(days=1)\n",
    "out = pd.DataFrame(columns=['track_id', 'skip_2', 'premium', 'hist_user_behavior_is_shuffle', 'context_type', 'total_count'])\n",
    "\n",
    "while start_date <= end_date:\n",
    "    current_date = start_date.strftime(\"%Y%m%d\")\n",
    "    start_date += delta\n",
    "        \n",
    "    #print(current_date + ' :start')\n",
    "    #For each date, filter and merge the data from all the files and join with track features\n",
    "    \n",
    "    for i in range(0,10):\n",
    "        #print('\\t \\t' + str(i) + ' :start')\n",
    "        df = pd.read_csv(work_dir+'training_set/log_'+str(i)+'_'+current_date+'_000000000000.csv')\n",
    "        df = df.rename({'track_id_clean': 'track_id'}, axis=1)\n",
    "        df = df[['track_id','skip_2', 'premium', 'hist_user_behavior_is_shuffle', 'context_type']]\n",
    "\n",
    "        df['skip_2'] = (df['skip_2'] == True ).astype(int)\n",
    "        df['premium'] = (df['premium'] == True ).astype(int)\n",
    "        df['hist_user_behavior_is_shuffle'] = (df['hist_user_behavior_is_shuffle'] == True ).astype(int)\n",
    "        \n",
    "        df.context_type.replace(to_replace=dict(editorial_playlist=0, user_collection=1, radio=2,personalized_playlist=3, catalog=4, charts=5), inplace=True)\n",
    "        \n",
    "        grouped = df.groupby(['track_id', 'skip_2', 'premium', 'hist_user_behavior_is_shuffle', 'context_type']).size().reset_index()\n",
    "        grouped = grouped.rename({0: 'total_count'}, axis=1)\n",
    "        \n",
    "        out = pd.merge(grouped, out, how='outer', on=['track_id', 'skip_2','premium', 'hist_user_behavior_is_shuffle', 'context_type'])\\\n",
    "        .set_index(['track_id','skip_2','premium', 'hist_user_behavior_is_shuffle', 'context_type'])\\\n",
    "        .sum(axis=1).reset_index()\n",
    "        \n",
    "        out = out.rename({0: 'count'}, axis=1)\n",
    "        #print('\\t \\t' + str(i) + ' :end')\n",
    "        pass\n",
    "    print(sum(out['count']))\n",
    "    print(current_date + ' :end')\n",
    "    pass\n",
    "\n",
    "out.to_csv(work_dir+'agg2018'+'.csv', header=True, sep=',') #Save file for backup so do not have to process this chunk again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset_selective df\n",
    "%reset_selective grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Sound track dataset and merge the files into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import and combine track features in a dataframe\n",
    "track0 = pd.read_csv(work_dir + 'track_features/tf_000000000000.csv')\n",
    "print(len(track0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track1 = pd.read_csv(work_dir + 'track_features/tf_000000000001.csv')\n",
    "print(len(track1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove records from 1950s to 1970s as the songs are heard by a very group of people\n",
    "track0 = track0[track0['release_year']>1970]\n",
    "print(len(track0))\n",
    "\n",
    "track1 = track1[track1['release_year']>1970]\n",
    "print(len(track1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_features = track0.append(track1, ignore_index=False, sort=False)\n",
    "#track_features['track_id'] = track_features['track_id'].astype(str)\n",
    "print(len(track_features))\n",
    "\n",
    "#Round all decimal values upto 4 decimal to decrease the dataset size\n",
    "track_features = track_features.round(decimals=4)\n",
    "\n",
    "track_features['mode'].replace(to_replace=dict(minor=0, major=1), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset_selective track0\n",
    "%reset_selective track1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join Tracks with User Session Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#out = pd.read_csv(work_dir+'agg2018.csv')\n",
    "\n",
    "jn = out.set_index('track_id').join(track_features.set_index('track_id'), on='track_id', how='inner').reset_index()\n",
    "\n",
    "jn.to_csv(work_dir+'final_dataset'+'.csv', header=True, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset_selective out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jn = pd.read_csv(work_dir+'final_dataset'+'.csv')\n",
    "print(jn.columns)\n",
    "jn = jn[['track_id', 'skip_2','premium', 'hist_user_behavior_is_shuffle', 'context_type',\n",
    "       'duration', 'release_year', 'us_popularity_estimate', 'acousticness',\n",
    "       'beat_strength', 'bounciness', 'danceability',\n",
    "       'energy', 'flatness', 'instrumentalness', 'liveness', 'loudness',\n",
    "       'mechanism', 'organism', 'speechiness', 'tempo', 'valence', 'count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = jn.groupby(['track_id'])['count'].sum().reset_index()\n",
    "count = count.rename({'count': 'total_count'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join = jn.set_index('track_id').join(count.set_index('track_id'), on='track_id', how='inner').reset_index()\n",
    "join.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join.to_csv(work_dir+'final_dataset_cleaned'+'.csv', header=True, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset_selective count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove all songs which have been heard less than 50 times in a month as they will stand out as outliers\n",
    "join = join[join['total_count']>49]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All the sound tracks which have been skipped and find count of those records\n",
    "skipped = join[join['skip_2']==1]\n",
    "skipped_count = skipped.groupby(['track_id', 'premium', 'hist_user_behavior_is_shuffle',\n",
    "       'context_type', 'duration', 'release_year', 'us_popularity_estimate',\n",
    "       'acousticness', 'beat_strength', 'bounciness', 'danceability', 'energy',\n",
    "       'flatness', 'instrumentalness', 'liveness', 'loudness', 'mechanism',\n",
    "       'organism', 'speechiness', 'tempo', 'valence'])['count'].sum().reset_index()\n",
    "skipped_count= skipped_count.rename({'count': 'skipped_count'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All the sound tracks which have been NOT been skipped and find count of those records\n",
    "not_skipped = join[join['skip_2']==0]\n",
    "not_skipped_count = not_skipped.groupby(['track_id', 'premium', 'hist_user_behavior_is_shuffle',\n",
    "       'context_type', 'duration', 'release_year', 'us_popularity_estimate',\n",
    "       'acousticness', 'beat_strength', 'bounciness', 'danceability', 'energy',\n",
    "       'flatness', 'instrumentalness', 'liveness', 'loudness', 'mechanism',\n",
    "       'organism', 'speechiness', 'tempo', 'valence'])['count'].sum().reset_index()\n",
    "not_skipped_count= not_skipped_count.rename({'count': 'not_skipped_count'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset_selective join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_counts = skipped_count.set_index(['track_id', 'premium', 'hist_user_behavior_is_shuffle',\n",
    "       'context_type', 'duration', 'release_year', 'us_popularity_estimate',\n",
    "       'acousticness', 'beat_strength', 'bounciness', 'danceability', 'energy',\n",
    "       'flatness', 'instrumentalness', 'liveness', 'loudness', 'mechanism',\n",
    "       'organism', 'speechiness', 'tempo', 'valence']).join(not_skipped_count.set_index(['track_id', 'premium', 'hist_user_behavior_is_shuffle',\n",
    "       'context_type', 'duration', 'release_year', 'us_popularity_estimate',\n",
    "       'acousticness', 'beat_strength', 'bounciness', 'danceability', 'energy',\n",
    "       'flatness', 'instrumentalness', 'liveness', 'loudness', 'mechanism',\n",
    "       'organism', 'speechiness', 'tempo', 'valence']), on=['track_id', 'premium', 'hist_user_behavior_is_shuffle',\n",
    "       'context_type', 'duration', 'release_year', 'us_popularity_estimate',\n",
    "       'acousticness', 'beat_strength', 'bounciness', 'danceability', 'energy',\n",
    "       'flatness', 'instrumentalness', 'liveness', 'loudness', 'mechanism',\n",
    "       'organism', 'speechiness', 'tempo', 'valence'], how='outer').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset_selective skipped_count\n",
    "%reset_selective not_skipped_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_counts.fillna(0,inplace=True)\n",
    "joined_counts['total_count']=joined_counts['skipped_count']+joined_counts['not_skipped_count']\n",
    "\n",
    "joined_counts.to_csv('final_agg.csv', header=True, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'joined_counts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-75d27d108085>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjoined_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'release_year_tr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'70s'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'release_year'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m1979\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'release_year_tr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'80s'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'release_year'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m1989\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'release_year_tr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'90s'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'release_year'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m1999\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'release_year_tr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'20s'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'release_year'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m2010\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'release_year_tr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'21s'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'joined_counts' is not defined"
     ]
    }
   ],
   "source": [
    "joined_counts['release_year_tr'] = '70s'\n",
    "joined_counts.loc[joined_counts['release_year']>1979, 'release_year_tr'] = '80s'\n",
    "joined_counts.loc[joined_counts['release_year']>1989, 'release_year_tr'] = '90s'\n",
    "joined_counts.loc[joined_counts['release_year']>1999, 'release_year_tr'] = '20s'\n",
    "joined_counts.loc[joined_counts['release_year']>2010, 'release_year_tr'] = '21s'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_counts.to_csv('final_agg_50_r_yr.csv', header=True, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
